name: Run Parallel Benchmarks in multiple clients

on:
  workflow_dispatch:
    inputs:
      test:
        description: 'Path to test file'
        default: 'eest_tests'
      warmup:
        description: 'Name of the warm up file'
        type: choice
        options:
          - ''
          - warmup/warmup-100bl-16wi-32tx.txt
          - warmup/warmup-1000bl-16wi-24tx.txt
          - warmup/warmup-1000bl-16wi-1000tx.txt
        default: ''
      client:
        description: 'Comma-separated list of client names'
        default: 'nethermind,geth,reth,besu,erigon,nimbus,ethrex'
      runs:
        description: 'Number of runs for the application'
        default: 1
      images:
        description: 'JSON map of images for the clients'
        default: '{"nethermind":"default","geth":"default","reth":"default","erigon":"default","besu":"default","nimbus":"default","ethrex":"default"}'
      opcodes_warmup_count:
        description: 'Number of opcode-warmup iterations'
        default: 1
      filter:
        description: 'Comma-separated include-only filename patterns'
        default: ''
      txt_report:
        description: 'Mark as true to generate txt report'
        default: 'false'
      genesis:
        description: 'Genesis filename (used for all clients)'
        default: 'zkevmgenesis.json'

jobs:
  set-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x

      - name: Set matrix dynamically
        id: set-matrix
        run: |
          runs=${{ github.event.inputs.runs }}
          clients="${{ github.event.inputs.client }}"
          IFS=',' read -ra client_array <<< "$clients"
          matrix_elements=()
          
          for ((i=1; i<=runs; i++)); do
            for client in "${client_array[@]}"; do
              element="{'run': '$i', 'client': '$client'}"
              matrix_elements+=("$element")
            done
          done
          
          matrix="{\"include\": [$(IFS=,; echo "${matrix_elements[*]}")]}"
          
          echo "::set-output name=matrix::$matrix"

  build:
    needs: set-matrix
    runs-on: ubuntu-latest

    strategy:
      # Consume the JSON matrix we just emitted
      matrix: ${{ fromJson(needs.set-matrix.outputs.matrix) }}
      fail-fast: false

    env:
      DOTNET_INSTALL_DIR: "~/.dotnet"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Setup Python & .NET
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Make run.sh executable
        run: chmod +x ./run.sh

      - name: Run benchmarks for client=${{ matrix.client }}
        run: |
          GENESIS_FILENAME="${{ github.event.inputs.genesis }}"
          TEST_PATH="${{ github.event.inputs.test }}"
          CLIENT="${{ matrix.client }}"
          TEST_PATHS_JSON="[ {\"path\": \"${TEST_PATH}\", \"genesis\": \"${GENESIS_FILENAME}\"} ]"
      
          ./run.sh \
            -T "$TEST_PATHS_JSON" \
            -w "${{ github.event.inputs.warmup }}" \
            -c "$CLIENT" \
            -r 1 \
            -i '${{ github.event.inputs.images }}' \
            -o "${{ github.event.inputs.opcodes_warmup_count }}" \
            -f "${{ github.event.inputs.filter }}"

      - name: Dump latest Docker logs for ${{ matrix.client }}
        if: always()
        run: |
          # Find the newest docker log file for this client
          LATEST=$(ls -1t logs/docker_${{ matrix.client }}_*.log | head -n1 || true)
          LATEST_SYNC=$(ls -1t logs/docker_sync_${{ matrix.client }}_*.log | head -n1 || true)
          if [[ -n "$LATEST" ]]; then
            echo "=== Dumping $LATEST ==="
            cat "$LATEST"
          else
            echo "No docker_*.log files found for client=${{ matrix.client }}"
          fi

          if [[ -n "$LATEST_SYNC" ]]; then
            echo "=== Dumping $LATEST_SYNC ==="
            cat "$LATEST_SYNC"
          else
            echo "No docker_sync_*.log files found for client=${{ matrix.client }}"
          fi

      - name: Zip the results folder
        run: |
          CLEANED_RUN=$(echo "${{ matrix.run }}" | tr -d '\n')
          CLEANED_CLIENT=$(echo "${{ matrix.client }}" | tr -d '\n')
          echo "CLEANED_RUN=$CLEANED_RUN" >> $GITHUB_ENV
          echo "CLEANED_CLIENT=$CLEANED_CLIENT" >> $GITHUB_ENV
          zip -r results-${CLEANED_RUN}-${CLEANED_CLIENT}.zip results

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}
          path: results-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}.zip
  combine-results:
    needs: build
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Set up .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Install python dependencies
        run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          path: combined-results
          merge-multiple: true

      - name: Extract all result files
        run: |
          mkdir -p extracted-results
          find combined-results -name '*.zip' -exec unzip -o {} -d extracted-results \;

      - name: Process combined results
        run: |
          set -euo pipefail
          RESULTS_ROOT="extracted-results/results"
          if [ ! -d "$RESULTS_ROOT" ]; then
            echo "Results directory not found: $RESULTS_ROOT"
            exit 1
          fi

          mapfile -t CLIENTS < <(find "$RESULTS_ROOT" -maxdepth 1 -type f -name '*_results_*')
          if [ "${#CLIENTS[@]}" -eq 0 ]; then
            echo "No client result files detected under $RESULTS_ROOT; skipping report generation."
            mkdir -p reports
            exit 0
          fi

          CLIENT_LIST=$(
            printf "%s\n" "${CLIENTS[@]}" \
              | xargs -n1 basename \
              | sed 's/_results_.*//' \
              | sort -u \
              | paste -sd',' -
          )

          if [ -z "$CLIENT_LIST" ]; then
            echo "Unable to derive client list from results; skipping report generation."
            mkdir -p reports
            exit 0
          fi

          rm -rf reports
          mkdir -p reports

          RUNS="${{ github.event.inputs.runs }}"
          TESTS="${{ github.event.inputs.test }}"
          IMAGES='${{ github.event.inputs.images }}'

          python3 report_tables.py --resultsPath "$RESULTS_ROOT" --clients "$CLIENT_LIST" --testsPath "$TESTS" --runs "$RUNS" --images "$IMAGES"
          python3 report_html.py   --resultsPath "$RESULTS_ROOT" --clients "$CLIENT_LIST" --testsPath "$TESTS" --runs "$RUNS" --images "$IMAGES"

      - name: Generate Report
        if: ${{ github.event.inputs.txt_report == 'true' }}
        run: |
          python3 report_txt.py --resultsPath extracted-results/results/ --clients "${{ github.event.inputs.client }}" --testsPath ${{ github.event.inputs.test }} 

      - name: Zip the results folder
        run: |
          zip -r reports.zip reports

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: reports
          path: reports.zip

  populate-db:
    needs: combine-results
    runs-on: ubuntu-latest
    env:
      DB_HOST: ${{ secrets.PERFNET_0_DB_HOST }}
      DB_PORT: ${{ secrets.PERFNET_0_DB_PORT || '5432' }}
      DB_USER: ${{ secrets.PERFNET_0_DB_USER }}
      DB_PASSWORD: ${{ secrets.PERFNET_0_DB_PASSWORD }}
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install python dependencies for submodule
        run: |
          pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          path: combined-results # Download artifacts from previous jobs
          merge-multiple: true

      - name: Extract all result files
        run: |
          mkdir -p extracted-results
          find combined-results -name '*.zip' -exec unzip -o {} -d extracted-results \;

      - name: Populate Benchmark DB
        run: |
          python fill_postgres_db.py \
            --db-host ${{ env.DB_HOST }} \
            --db-port ${{ env.DB_PORT }} \
            --db-user ${{ env.DB_USER }} \
            --db-name monitoring \
            --table-name gas_benchmarks_ci \
            --db-password "${{ env.DB_PASSWORD }}" \
            --log-level DEBUG \
            --reports-dir 'extracted-results/reports/'
