name: Run Parallel Benchmarks in multiple clients

on:
  workflow_dispatch:
    inputs:
      test:
        description: 'Path to test file'
        default: 'eest_tests'
      warmup:
        description: 'Name of the warm up file'
        type: choice
        options:
          - ''
          - warmup/warmup-100bl-16wi-32tx.txt
          - warmup/warmup-1000bl-16wi-24tx.txt
          - warmup/warmup-1000bl-16wi-1000tx.txt
        default: ''
      client:
        description: 'Comma-separated list of client names'
        default: 'nethermind,geth,reth,besu,erigon,nimbus,ethrex'
      runs:
        description: 'Number of runs for the application'
        default: 1
      images:
        description: 'JSON map of images for the clients'
        default: '{"nethermind":"default","geth":"default","reth":"default","erigon":"default","besu":"default","nimbus":"default","ethrex":"default"}'
      opcodes_warmup_count:
        description: 'Number of opcode-warmup iterations'
        default: 1
      filter:
        description: 'Comma-separated include-only filename patterns'
        default: ''
      txt_report:
        description: 'Mark as true to generate txt report'
        default: 'false'
      genesis:
        description: 'Genesis filename (used for all clients)'
        default: 'zkevmgenesis.json'

jobs:
  set-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      max_parallel: ${{ steps.set-matrix.outputs.max_parallel }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Set matrix dynamically with sharded tests
        id: set-matrix
        env:
          RUNS: ${{ github.event.inputs.runs }}
          CLIENTS: ${{ github.event.inputs.client }}
          TEST_PATH: ${{ github.event.inputs.test }}
          GENESIS: ${{ github.event.inputs.genesis }}
          FILTER: ${{ github.event.inputs.filter }}
        run: |
            python3 <<'PY'
            import json
            import math
            import os
            import pathlib
            import sys


            def parse_positive_int(raw_value: str, name: str, default: int = 1) -> int:
                try:
                    value = int(str(raw_value))
                except ValueError:
                    print(f"::warning title=Invalid {name} input::{name} must be an integer; defaulting to {default}")
                    return default
                if value < 1:
                    print(f"::warning title=Invalid {name} input::{name} must be >=1; defaulting to {default}")
                    return default
                return value


            runs = parse_positive_int(os.environ.get("RUNS", "1"), "runs")
            clients = [c.strip() for c in os.environ.get("CLIENTS", "").split(",") if c.strip()]
            test_path = os.environ.get("TEST_PATH", "").strip()
            genesis = os.environ.get("GENESIS", "").strip()
            filter_raw = os.environ.get("FILTER", "")
            filters = [f.strip().lower() for f in filter_raw.split(",") if f.strip()]
            matrix_cap = 256  # GitHub matrix include limit

            if runs < 1:
                print("::warning title=Invalid runs input::Runs must be >=1; defaulting to 1")
                runs = 1
            if not clients:
                print("::error title=No clients found::Client list is empty")
                sys.exit(1)
            if runs * len(clients) > matrix_cap:
                print(f"::error title=Matrix too large::runs x clients ({runs} x {len(clients)}) exceeds {matrix_cap}. Reduce inputs or run fewer clients.")
                sys.exit(1)
            if not test_path:
                print("::error title=Missing test path::Input 'test' is required")
                sys.exit(1)

            path = pathlib.Path(test_path)
            tests = []
            if path.is_file():
                tests = [str(path)]
            elif path.is_dir():
                tests = [str(p) for p in sorted(path.rglob("*.txt")) if p.is_file()]
            else:
                print(f"::error title=Invalid test path::Test path not found: {test_path}")
                sys.exit(1)

            if not tests:
                print(f"::error title=No tests found::No test files discovered under {test_path}")
                sys.exit(1)

            if filters:
                filtered_tests = []
                for t in tests:
                    name_l = pathlib.Path(t).name.lower()
                    if any(pat in name_l for pat in filters):
                        filtered_tests.append(t)
                tests = filtered_tests
                if not tests:
                    print(f"::error title=Filter eliminated tests::No tests matched filter(s): {', '.join(filters)}")
                    sys.exit(1)

            max_shards_allowed = matrix_cap // (runs * len(clients))
            if max_shards_allowed < 1:
                print(f"::error title=Matrix too large::runs x clients ({runs} x {len(clients)}) exceeds {matrix_cap}. Reduce inputs or run fewer clients.")
                sys.exit(1)

            shard_count = min(len(tests), max_shards_allowed)
            chunk_size = math.ceil(len(tests) / shard_count)

            shards = []
            for idx in range(shard_count):
                shard_tests = tests[idx * chunk_size : (idx + 1) * chunk_size]
                if not shard_tests:
                    continue
                shard_label = f"shard-{idx + 1:03d}-of-{shard_count:03d}"
                shards.append(
                    {
                        "shard_index": idx + 1,
                        "shard_total": shard_count,
                        "shard_label": shard_label,
                        "tests_json": json.dumps([{"path": t, "genesis": genesis} for t in shard_tests]),
                    }
                )

            matrix_include = []
            for run_idx in range(1, runs + 1):
                for client in clients:
                    for shard in shards:
                        entry = {"run": str(run_idx), "client": client}
                        entry.update(shard)
                        matrix_include.append(entry)

            matrix = json.dumps({"include": matrix_include})
            max_parallel = min(matrix_cap, len(matrix_include)) or 1

            with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as fh:
                fh.write(f"matrix={matrix}\n")
                fh.write(f"max_parallel={max_parallel}\n")

            print(f"Auto-sharded {len(tests)} tests into {len(shards)} shards (runs={runs}, clients={len(clients)}); total jobs: {len(matrix_include)}.")
            PY

  build:
    needs: set-matrix
    runs-on: ubuntu-latest

    strategy:
      # Consume the JSON matrix we just emitted
      matrix: ${{ fromJson(needs.set-matrix.outputs.matrix) }}
      fail-fast: false
      max-parallel: ${{ fromJson(needs.set-matrix.outputs.max_parallel) }}

    env:
      DOTNET_INSTALL_DIR: "~/.dotnet"
      SHARD_LABEL: ${{ matrix.shard_label }}
      SHARD_INDEX: ${{ matrix.shard_index }}
      SHARD_TOTAL: ${{ matrix.shard_total }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Setup Python & .NET
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Make run.sh executable
        run: chmod +x ./run.sh

      - name: Run benchmarks for client=${{ matrix.client }}
        env:
          TEST_PATHS_JSON: ${{ matrix.tests_json }}
          CLIENT_ARTIFACTS_DIR: ${{ format('{0}/client-artifacts/{1}/{2}', github.workspace, matrix.client, matrix.shard_label) }}
        run: |
          GENESIS_FILENAME="${{ github.event.inputs.genesis }}"
          CLIENT="${{ matrix.client }}"
          rm -rf "$CLIENT_ARTIFACTS_DIR" || sudo rm -rf "$CLIENT_ARTIFACTS_DIR" || true
          mkdir -p "$CLIENT_ARTIFACTS_DIR"
      
          ./run.sh \
            -T "$TEST_PATHS_JSON" \
            -X \
            -w "${{ github.event.inputs.warmup }}" \
            -c "$CLIENT" \
            -r 1 \
            -i '${{ github.event.inputs.images }}' \
            -o "${{ github.event.inputs.opcodes_warmup_count }}" \
            -f "${{ github.event.inputs.filter }}"

      - name: Dump latest Docker logs for ${{ matrix.client }}
        if: always()
        run: |
          # Find the newest docker log file for this client
          LATEST=$(ls -1t logs/docker_${{ matrix.client }}_*.log | head -n1 || true)
          LATEST_SYNC=$(ls -1t logs/docker_sync_${{ matrix.client }}_*.log | head -n1 || true)
          if [[ -n "$LATEST" ]]; then
            echo "=== Dumping $LATEST ==="
            cat "$LATEST"
          else
            echo "No docker_*.log files found for client=${{ matrix.client }}"
          fi

          if [[ -n "$LATEST_SYNC" ]]; then
            echo "=== Dumping $LATEST_SYNC ==="
            cat "$LATEST_SYNC"
          else
            echo "No docker_sync_*.log files found for client=${{ matrix.client }}"
          fi

      - name: Zip the results folder
        run: |
          CLEANED_RUN=$(echo "${{ matrix.run }}" | tr -d '\n')
          CLEANED_CLIENT=$(echo "${{ matrix.client }}" | tr -d '\n')
          CLEANED_SHARD=$(echo "${{ env.SHARD_LABEL }}" | tr -d '\n')
          echo "CLEANED_RUN=$CLEANED_RUN" >> $GITHUB_ENV
          echo "CLEANED_CLIENT=$CLEANED_CLIENT" >> $GITHUB_ENV
          echo "CLEANED_SHARD=$CLEANED_SHARD" >> $GITHUB_ENV
          zip -r results-${CLEANED_RUN}-${CLEANED_CLIENT}-${CLEANED_SHARD}.zip results

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}-${{ env.CLEANED_SHARD }}
          path: results-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}-${{ env.CLEANED_SHARD }}.zip
          compression-level: 6
      
      - name: Zip client outputs
        if: always()
        env:
          CLIENT_ARTIFACTS_DIR: ${{ format('{0}/client-artifacts/{1}/{2}', github.workspace, matrix.client, matrix.shard_label) }}
          CLIENT_ARTIFACTS_RELATIVE: ${{ format('client-artifacts/{0}/{1}', matrix.client, matrix.shard_label) }}
          OUTPUT_ZIP: outputs-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}-${{ env.CLEANED_SHARD }}.zip
        run: |
          set -euo pipefail
          rm -f "$OUTPUT_ZIP"
          if [ -d "$CLIENT_ARTIFACTS_DIR" ] && [ "$(ls -A "$CLIENT_ARTIFACTS_DIR")" ]; then
            zip -r "$OUTPUT_ZIP" "$CLIENT_ARTIFACTS_RELATIVE"
          else
            echo "No client outputs found in $CLIENT_ARTIFACTS_DIR"
            mkdir -p client-artifacts-placeholder
            zip -r "$OUTPUT_ZIP" client-artifacts-placeholder
          fi

      - name: Upload client outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: client-outputs-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}-${{ env.CLEANED_SHARD }}
          path: outputs-${{ env.CLEANED_RUN }}-${{ env.CLEANED_CLIENT }}-${{ env.CLEANED_SHARD }}.zip
          compression-level: 6

  combine-results:
    needs: build
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Set up .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Install python dependencies
        run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          path: combined-results
          merge-multiple: true

      - name: Extract all result files
        run: |
          mkdir -p extracted-results
          find combined-results -name '*.zip' -print0 | xargs -0 -n1 -P "$(nproc)" unzip -o -d extracted-results

      - name: Create merged traces artifact (all shards)
        if: always()
        run: |
          tar -czf all-traces.tar.gz -C extracted-results .

      - name: Upload merged traces
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: all-traces
          path: all-traces.tar.gz
          compression-level: 6

      - name: Process combined results
        run: |
          set -euo pipefail
          RESULTS_ROOT="extracted-results/results"
          if [ ! -d "$RESULTS_ROOT" ]; then
            echo "Results directory not found: $RESULTS_ROOT"
            exit 1
          fi

          mapfile -t CLIENTS < <(find "$RESULTS_ROOT" -maxdepth 1 -type f -name '*_results_*')
          if [ "${#CLIENTS[@]}" -eq 0 ]; then
            echo "No client result files detected under $RESULTS_ROOT; skipping report generation."
            mkdir -p reports
            exit 0
          fi

          CLIENT_LIST=$(
            printf "%s\n" "${CLIENTS[@]}" \
              | xargs -n1 basename \
              | sed 's/_results_.*//' \
              | sort -u \
              | paste -sd',' -
          )

          if [ -z "$CLIENT_LIST" ]; then
            echo "Unable to derive client list from results; skipping report generation."
            mkdir -p reports
            exit 0
          fi

          rm -rf reports
          mkdir -p reports

          RUNS="${{ github.event.inputs.runs }}"
          TESTS="${{ github.event.inputs.test }}"
          IMAGES='${{ github.event.inputs.images }}'

          python3 report_tables.py --resultsPath "$RESULTS_ROOT" --clients "$CLIENT_LIST" --testsPath "$TESTS" --runs "$RUNS" --images "$IMAGES"
          python3 report_html.py   --resultsPath "$RESULTS_ROOT" --clients "$CLIENT_LIST" --testsPath "$TESTS" --runs "$RUNS" --images "$IMAGES"

      - name: Generate Report
        if: ${{ github.event.inputs.txt_report == 'true' }}
        run: |
          python3 report_txt.py --resultsPath extracted-results/results/ --clients "${{ github.event.inputs.client }}" --testsPath ${{ github.event.inputs.test }} 

      - name: Zip the results folder
        run: |
          zip -r reports.zip reports

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: reports
          path: reports.zip
          compression-level: 6

      - name: Cleanup merged artifacts workspace
        if: always()
        run: |
          rm -rf combined-results extracted-results reports reports.zip all-traces.tar.gz || true

  populate-db:
    needs: combine-results
    runs-on: ubuntu-latest
    env:
      DB_HOST: ${{ secrets.PERFNET_0_DB_HOST }}
      DB_PORT: ${{ secrets.PERFNET_0_DB_PORT || '5432' }}
      DB_USER: ${{ secrets.PERFNET_0_DB_USER }}
      DB_PASSWORD: ${{ secrets.PERFNET_0_DB_PASSWORD }}
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install python dependencies for submodule
        run: |
          pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          path: combined-results # Download artifacts from previous jobs
          merge-multiple: true

      - name: Extract all result files
        run: |
          mkdir -p extracted-results
          find combined-results -name '*.zip' -exec unzip -o {} -d extracted-results \;

      - name: Populate Benchmark DB
        run: |
          python fill_postgres_db.py \
            --db-host ${{ env.DB_HOST }} \
            --db-port ${{ env.DB_PORT }} \
            --db-user ${{ env.DB_USER }} \
            --db-name monitoring \
            --table-name gas_benchmarks_ci \
            --db-password "${{ env.DB_PASSWORD }}" \
            --log-level DEBUG \
            --reports-dir 'extracted-results/reports/'
